<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Numpy vs. Python Lists | GK&#39;s Blog</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>

  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/post">Blog</a></li>
      
      <li><a href="/note">Note</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/projects/">Projects</a></li>
      
      <li><a href="/resume.pdf">Resume</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="http://amzn.in/j8KlRjG">Amazon-books</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
    <h1><span class="title">Numpy vs. Python Lists</span></h1>
    
    <h2 class="date">2018/02/05</h2>
</div>

<main>
    

<p>While I was learning Machine Learning, I made my mind not to use any of the third-party-libraries (including numpy and Pandas) until I got to know what  happened in the background and feel okay to use libraries.
<!-- more --></p>

<div align="center">
    <br />
  <img src="https://www.dropbox.com/s/dxtooh0s29f0v8u/numpy-python.png?raw=1"><br><br>
</div>

<p>It was a bit harder to write everything from scratch. Even the <code>dot product</code> must be written, cause python doesn&rsquo;t have that inbuilt.</p>

<p>Finally, I&rsquo;ve felt comfortable and could clearly understand what is happening in the background (cause, Machine Learning is all about Math).</p>

<h1 id="implementing-stochastic-gradient-descent">Implementing Stochastic Gradient Descent</h1>

<p>Stochastic gradient descent is actually scanning through the training examples, and then it&rsquo;ll take a little gradient descent step with respect to the cost function of just that training example.</p>

<p>The cost function measures how well the hypothesis is doing on a single example.</p>

<div align="center">

$${cost( \Theta, (x^{(i)}, y^{(i)})) = \dfrac{1}{2}(h_\Theta(x^{(i)}) - y^{(i)})^2}$$

$${J_{train}(\Theta)  = \dfrac{1}{m} \sum_{i=1}^{m} cost( \Theta, (x^{(i)}, y^{(i)}))}$$

</div>

<h2 id="a-dumb-stochastic-gradient-descent-implementation">A dumb Stochastic Gradient Descent Implementation</h2>

<h3 id="using-lists">Using Lists:</h3>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#00f">def</span> dot(v, w):
    <span style="color:#00f">return</span> sum(v_i * w_i <span style="color:#00f">for</span> v_i, w_i <span style="color:#00f">in</span> zip(v, w))

<span style="color:#00f">def</span> predict_multi(x_i, beta):
    <span style="color:#00f">return</span> dot(x_i, beta)

<span style="color:#00f">def</span> error_multi(beta, x, y):
    <span style="color:#00f">return</span> y - predict_multi(x, beta)

<span style="color:#00f">def</span> stochastic_gradient_descent(x, y, theta, learning_rate=0.001):
    <span style="color:#00f">for</span> _ <span style="color:#00f">in</span> range(1500):
        <span style="color:#00f">for</span> x_i, y_i <span style="color:#00f">in</span> zip(x, y):
            i = 0
            <span style="color:#00f">for</span> x_ii, theta_i <span style="color:#00f">in</span> zip(x_i, theta):
                theta[i] = theta_i - learning_rate * (-error_multi(theta, x_i, y_i)) * x_ii
                i += 1
    <span style="color:#00f">return</span> theta</code></pre></div>
<p>So, the benchmark results</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-md" data-lang="md">%timeit stochastic_gradient_descent(multi_data, daily_minutes_good, theta)<span style="">
</span><span style=""></span>1.85 s ± 21.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre></div>
<h3 id="using-numpy">Using Numpy:</h3>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#00f">import</span> numpy <span style="color:#00f">as</span> np

<span style="color:#00f">def</span> stochastic_gradient_descent_np(x, y, theta, learning_rate=0.001):
    <span style="color:#00f">for</span> _ <span style="color:#00f">in</span> range(1500):
        <span style="color:#00f">for</span> x_i, y_i <span style="color:#00f">in</span> zip(x, y):
            theta = theta - learning_rate * (np.sum(x_i * theta) - y_i) * x_i
    <span style="color:#00f">return</span> theta</code></pre></div>
<p>That&rsquo;s it, boom. The Numpy implementation of Gradient Descent actually uses a vectorized implementation.</p>

<p>Benchmark using Numpy:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-md" data-lang="md">%timeit stochastic_gradient_descent_np(np_multi_data, np_daily, np_theta)<span style="">
</span><span style=""></span>1.96 s ± 7.53 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre></div>
<p>Using Numpy doesn&rsquo;t just make the implementation simpler but also faster.</p>

</main>
  <footer>
  <script>renderMathInElement(document.body);</script>
<script>hljs.initHighlightingOnLoad();</script>
<script>renderMathInElement(document.body);</script>

  
  <hr/>
  &copy; <a href="http://www.gnaneshkunal.com">Gnanesh Kunal</a> 2018 | <a href="https://github.com/gnaneshkunal">Github</a> | <a href="mailto:gnaneshkunal@outlook.com">Mail</a> | <a href="https://twitter.com/gnaneshkunal">Twitter</a>
  
  </footer>
  </body>
</html>

